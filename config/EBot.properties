### the parameters below are related to the reinforcement learning algorithm ###
# specifies the type of learning agent
rl.agent = "sarsa"

# the initial value and decay of exploration rate (epsilon is multiplied by this decay factor after each episode)
rl.epsilon.initial = 0.3
rl.epsilon.decay = 0.9998			

# the initial value and decay of learning rate (alpha is multiplied by this decay factor after each episode)
rl.alpha.initial = 0.0001
rl.alpha.decay = 1

# Note: setting the decay rates to 1 makes the parameters constant throughout all episodes

# the discount factor
rl.gamma = 0.9

# eligibility trace (not used yet)
rl.lambda = 0

# the feature extractor
rl.feature.extractor = quadrant_model

# the map is divided in quadrant_division x quadrant_division quadrants
# this parameter is specific of the quadrant_model
rl.feature.extractor.quadrant_division = 3

# the random seed (if not specified, it will load the default seed)
rl.random.seed = 1

rl.workingdir = test/

rl.save_weights_bin = True

# # the prefix of the output file to save weights
rl.output.binprefix = training/weightstore/output/

# # test load weight
rl.load_weights_bin = False
#rl.bin_input = training/weightstore/input/weightstore_1.bin
rl.bin_input = training/weightstore/output/weightstore.bin

#debug setting
rl.debug = False
# # the prefix of the debug dir
rl.debugdir = debug/
